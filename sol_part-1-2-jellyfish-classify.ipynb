{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa573da-6fb3-4f81-86ce-6cef8c1e1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5797477-8a71-490b-8b8f-72f4b71acfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f3acc-d405-4858-af55-64158eabe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ad729-6661-44e3-a3f8-21023f09b90c",
   "metadata": {},
   "source": [
    "## 1. Daten Encodiern und Laden\n",
    "\n",
    "In PyTorch mit einem Dataset + Dataloaders. Zusätzlich brauchen wir noch Funktionen um die Klassen der Quallen zu encodieren + decodieren.\n",
    "\n",
    "1. Encoding / decoding Funktionen implementieren\n",
    "2. Database Klasse erstellen (erstmal nur train+val) ALTERNATIV ImageFolder von torchvision.datasets nutzen, falls ja wie Abwägung begründen\n",
    "3. Was machen die Vordefinierten transformationen?\n",
    "4. Dataloaders definieren.\n",
    "    - WICHTIG, shuffle flag nicht vergessen. Warum?\n",
    "    - train batchsize setzen. Warum? Welche tradeoffs bei hoher / niedriger batchsize\n",
    "    - Brauchen wir batchsize & shuffle flag bei train und val?\n",
    "\n",
    "### Hinweise\n",
    "- Was genau braucht die loss function später? Wie kommen wir von dem String der Quallen Klassen zu dem was die loss function braucht?\n",
    "- Dataset Doku: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "    - ImageFolder Doku: https://pytorch.org/vision/0.16/generated/torchvision.datasets.ImageFolder.html?highlight=imagefolder#torchvision.datasets.ImageFolder\n",
    "- Ein Dataset braucht eine Liste mit informationen (Image location + Klasse)\n",
    "- `read_image` von `torchvision.io` erstellt lädt ein Bild als Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90277f10-789d-4275-a8a4-ab9576aaf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper code\n",
    "\"\"\"\n",
    "\n",
    "data_dir_train = 'data/jellyfish/Train_Test_Valid/Train'\n",
    "data_dir_val = 'data/jellyfish/Train_Test_Valid/valid'\n",
    "\n",
    "labels_train = []\n",
    "labels_val = []\n",
    "\n",
    "def get_last_folder(path):\n",
    "    return os.path.basename(os.path.normpath(path))\n",
    "\n",
    "def get_labels(root_dir):\n",
    "    res = []\n",
    "    for s in glob.glob(root_dir + '/*'):\n",
    "        cls = get_last_folder(s)\n",
    "        for f in glob.glob(s + '/*'):\n",
    "            res.append((cls, f))\n",
    "    return res\n",
    "\n",
    "labels_train = get_labels(data_dir_train)\n",
    "labels_val = get_labels(data_dir_val)\n",
    "\n",
    "\n",
    "transforms_train = v2.Compose([\n",
    "    v2.Resize((244, 244)),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    # v2.RandomRotation(degrees=(-20, 20)),\n",
    "    v2.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    v2.RandomErasing(p=0.5, scale=(0.1,0.15)),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "transforms_val = v2.Compose([\n",
    "    v2.Resize((244, 244)),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b0df-e68f-4cbe-878c-18129d804ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mapping = [\n",
    "    'barrel_jellyfish',\n",
    "    'blue_jellyfish',\n",
    "    'compass_jellyfish',\n",
    "    'lions_mane_jellyfish',\n",
    "    'mauve_stinger_jellyfish',\n",
    "    'Moon_jellyfish'\n",
    "]\n",
    "encoding_mapping = { c: idx for idx, c in enumerate(cls_mapping) }\n",
    "decoding_mapping = { idx: c for idx, c in enumerate(cls_mapping) }\n",
    "\n",
    "def encode(raw_cls):\n",
    "    return encoding_mapping[raw_cls]\n",
    "\n",
    "def decode(encoded_cls):\n",
    "    return decoding_mapping[encoded_cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370c377-e22d-4701-91a1-d249fc2c5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JellifishDS(Dataset):\n",
    "    labels = []\n",
    "    transforms = []\n",
    "    \n",
    "    def __init__(self, labels, transforms=None):\n",
    "        \"\"\"\n",
    "        labels is a list of (raw_jellifish_class, file_path)\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        raw_cls, img_path = self.labels[idx]\n",
    "\n",
    "        img_tensor = read_image(img_path)\n",
    "        img_tensor = img_tensor.float() / 255.0\n",
    "\n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "\n",
    "        encoded_cls = encode(raw_cls)\n",
    "        \n",
    "        return img_tensor, encoded_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246704ec-7c82-45a7-aed0-5d94d1d16ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = JellifishDS(labels_train, transforms=transforms_train)\n",
    "ds_valid = JellifishDS(labels_val, transforms=transforms_val)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=16)\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "    'train': dl_train,\n",
    "    'val': dl_valid\n",
    "}\n",
    "\n",
    "print('input shape dataset:', ds_train[0][0].shape)\n",
    "i = next(iter(dl_train))\n",
    "print('input shape dataloader train batch:', i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90928723-5c2b-43a8-b819-003203a84c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    v2.Resize((244,244))\n",
    "])\n",
    "\n",
    "ds_train_1 = datasets.ImageFolder(root=data_dir_train, transform=transforms)\n",
    "ds_valid_1 = datasets.ImageFolder(root=data_dir_val, transform=transforms)\n",
    "ds_train_1[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995eda3-6d53-420e-95dd-057c2fdaa0d2",
   "metadata": {},
   "source": [
    "## 2. Loss function und Model Definieren\n",
    "- Wie ermitteln wir wie gut unsere Vorhersage ist? Wie vermitteln wir unserem Model wie es sich verbessern soll? \n",
    "- Wir machen Transfer learning. Dafür brauchen wir ein vortainiertes Resnet oder ähnliches Model.\n",
    "    - Das Grundlegende Model müssen wir anpassen um einen Output mit passender 'shape' zu generieren. Was bedeutet das genau?\n",
    "- Printe das Model mit torchsummary `print(summary(model, input_size=(3,244,244), device='cpu'))`. Was sieht man?\n",
    "### Hinweise\n",
    "- Um vortrainierte Modelle einzubinden gibt es die torchvision (`torchvision.models`) oder timm library\n",
    "    - z.B. `from torchvision.models import efficientnet_v2_s, EfficientNet` oder `import timm => timm.create_model(...)`\n",
    "    - bei timm kann man sich mit `timm.list_models()` eine übersicht loggen \n",
    "- Welchen Input braucht die loss function?\n",
    "      - der output des Models muss zu einem der Inputs der loss function passen. Der andere Input der loss function kommt durch den Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207e0dd-ea0a-4385-a824-327ceaac59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c85f45-1a64-45dd-ab69-cffd24bd664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af6c29-41b0-4c2e-9912-3a03e30a2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet, EfficientNet_V2_S_Weights, efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "class JellyNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.basemodel = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights)\n",
    "        self.basemodel = efficientnet_b0(weights=EfficientNet_B0_Weights)\n",
    "        \n",
    "        classifier = self.basemodel.classifier\n",
    "        #print(classifier)\n",
    "        \n",
    "        in_features = [m for m in classifier.modules()]\n",
    "        in_features = in_features[-1].in_features\n",
    "        \n",
    "        new_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        #print(new_classifier)\n",
    "        self.basemodel.classifier = new_classifier\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.basemodel(batch)\n",
    "\n",
    "model = JellyNet(6)\n",
    "print(summary(model, input_size=(3,244,244), device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4eea9b-c2db-49a9-9f98-e2ad5bc6657f",
   "metadata": {},
   "source": [
    "## Trail (+ Val Loop)\n",
    "- In purem PyTorch gibt es keine höhere Abstraktion für das trainieren/validieren. Daher müssen wir dafür etwas eigenes schreiben.\n",
    "- PyTorch kann nur dinge verrechnen die auf dem gleichen 'device' sind. Falls ein GPU verfügbar ist sollten wir diesen nutzen.\n",
    "\n",
    "#### Hinweise\n",
    "- ob ein GPU verfügbar ist findet man über `torch.cuda.is_available()` heraus.\n",
    "- Valide device ids sind die strings `cuda` und `cpu`.\n",
    "- `tensor.to(device)` schiebt den Tensor auf den device. `model.to(device)` schiebt die Gewichte des Models auf den device. Achtung, die tensor operation gibt eine neue referenz zurück. Sie ist nicht \"inplace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b334593-60ca-4ccb-b335-1a90979cd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_no_metrics(model, dataloaders, n_epochs, criterion, optimizer):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        dl = dataloaders['train']\n",
    "        avg_loss = 0.0\n",
    "        acc = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in tqdm(enumerate(dl), total=len(dl)):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss\n",
    "        avg_loss /= (i+1)\n",
    "        loss_train.append(avg_loss)\n",
    "\n",
    "        print(f'{epoch:3g}-trai:\\tavg_loss: {avg_loss:.3f}')\n",
    "        avg_loss = 0.0\n",
    "        \n",
    "        dl = dataloaders['val']\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, val_data in tqdm(enumerate(dl), total=len(dl)):\n",
    "                val_inputs, val_labels = val_data\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                avg_loss +=  criterion(val_outputs, val_labels)\n",
    "            avg_loss /= (i+1)\n",
    "            print(f'{epoch:3g}-val:\\tavg_loss: {avg_loss:.3f}')\n",
    "            loss_val.append(avg_loss)\n",
    "\n",
    "    return {\n",
    "        'loss_train': loss_train,\n",
    "        'loss_val': loss_val\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6863d1-8adc-43ae-8bd6-63f76883572d",
   "metadata": {},
   "source": [
    "## 3. Optimizer + Hyperparameter definieren\n",
    "- Was macht ein Optimizer?\n",
    "- Welche Optimizer gibt es? Welchen nehmen wir?\n",
    "- Welche Hyperparameter gibt es noch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca37f9-72de-4620-8b72-de20afa89270",
   "metadata": {},
   "source": [
    "## 4. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1ad31-8bfb-4c33-996f-886b4f0f80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "res = train_no_metrics(\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    n_epochs=n_epochs,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6ba47-f074-40cb-9cd1-ddceb4e8e603",
   "metadata": {},
   "source": [
    "## Hinzufügen von metrics\n",
    "- Loss bestimmt die Mathematische Qualität. Klassifizierungsmetriken die Praktische.\n",
    "    - Uns reicht erstmal Multiclass accuracy\n",
    "- Beobachte was passiert\n",
    "- Was können wir alles ändern / anpassen. Mit welcher Begründung?\n",
    "\n",
    "### Hinweise\n",
    "- Für Metriken gibt es verschiedene Libraries. Wir könnten mit https://pytorch.org/torcheval/main/generated/torcheval.metrics.MulticlassAccuracy.html#torcheval.metrics.MulticlassAccuracy starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700969a2-78b3-451e-8624-f55d1cfc1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf6029-4cca-4b49-8289-eb0813820950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "def train(model, dataloaders, n_epochs, criterion, optimizer):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    metric = MulticlassAccuracy()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        dl = dataloaders['train']\n",
    "        avg_loss = 0.0\n",
    "        acc = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in tqdm(enumerate(dl), total=len(dl)):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metric.update(output, labels)\n",
    "            avg_loss += loss\n",
    "        avg_loss /= (i+1)\n",
    "        loss_train.append(avg_loss)\n",
    "        acc = metric.compute()\n",
    "        metric.reset()\n",
    "        acc_train.append(acc)\n",
    "\n",
    "        print(f'{epoch:3g}-trai:\\tavg_loss: {avg_loss:.3f} acc: {acc:.3f}')\n",
    "        avg_loss = 0.0\n",
    "        \n",
    "        dl = dataloaders['val']\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, val_data in tqdm(enumerate(dl), total=len(dl)):\n",
    "                val_inputs, val_labels = val_data\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                avg_loss +=  criterion(val_outputs, val_labels)\n",
    "                metric.update(val_outputs, val_labels)\n",
    "            avg_loss /= (i+1)\n",
    "            loss_val.append(avg_loss)\n",
    "            acc = metric.compute()\n",
    "            acc_val.append(acc)\n",
    "            metric.reset()\n",
    "    \n",
    "            print(f'{epoch:3g}-val:\\tavg_loss: {avg_loss:.3f} acc: {acc:.3f}')\n",
    "\n",
    "    return {\n",
    "        'loss_train': loss_train,\n",
    "        'loss_val': loss_val,\n",
    "        'acc_train': acc_train,\n",
    "        'acc_val': acc_val,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff04e34-41e5-4d72-9fda-ff11c2220766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "n_epochs = 20\n",
    "lr = 0.0001\n",
    "model = JellyNet(6)\n",
    "#model = timm.create_model('resnet18', pretrained=True, num_classes=6)\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "res = train(\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    n_epochs=n_epochs,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f62140-76f0-41f3-98db-aa0580fdf4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
